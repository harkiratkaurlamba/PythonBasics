{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52e8b700-b002-4b1a-b6ac-5d30eb098371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Kafka\n",
    "\n",
    "- stream storage - no complex query model\n",
    "- no analytical capabilities\n",
    "- messages are note delted once consumed(unlike queues)\n",
    "- consumers can't sclae beyond the number of partitions in a topic\n",
    "- topics are immutable, append only logs\n",
    "- full history preservation is not there..chk this though\n",
    "\n",
    "challenge: A typical organization has multiple sources of data with disparate data formats. Data integration involves combining data from these multiple sources into one unified view of their business.\n",
    "\n",
    "\n",
    "why do we need kafka?\n",
    "1. Apache Kafka allows us to decouple data streams and systems.\n",
    "\n",
    "Definitions\n",
    "- Producer: Applications that send data into a topic. Kafka producers are deployed outside Kafka and only interact with Apache Kafka by sending data directly into the Kafka topics.\n",
    "- Consumer: \n",
    "- Topic: the categories used to organize messages. Kafka topics can contain any kind of message in any format, and the sequence of all these messages is called a data stream.\n",
    "Data in Kafka topics is deleted after one week by default (also called the default message retention period), and this value is configurable. This mechanism of deleting old data ensures a Kafka cluster does not run out of disk space by recycling topics over time. Kafka topics are not query-able. Kafka topics are immutable: once data is written to a partition, it cannot be changed.\n",
    "- Partitions: Topics are broken down into a number of partitions, 0 to N-1\n",
    "- Offset: The offset is an integer value that Kafka adds to each message as it is written into a partition. Each message in a given partition has a unique offset.\n",
    "If a topic has more than one partition, Kafka guarantees the order of messages within a partition, but there is no ordering of messages across partitions.\n",
    "Even though we know that messages in Kafka topics are deleted over time (as seen above), the offsets are not re-used. They continually are incremented in a never-ending sequence.\n",
    "\n",
    "- Broker:\n",
    "\n",
    "\n",
    "A traffic company wants to track its fleet of trucks. Each truck is fitted with a GPS locator that reports its position to Kafka. We can create a topic named - trucks_gps to which the trucks publish their positions. Each truck may send a message to Kafka every 20 seconds, each message will contain the truck ID and the truck position (latitude and longitude). The topic may be split into a suitable number of partitions, say 10. There may be different consumers of the topic. For example, an application that displays truck locations on a dashboard or another application that sends notifications if an event of interest occurs.\n",
    "\n",
    "Message Keys\n",
    "Each event message contains an optional key and a value.\n",
    "\n",
    "In case the key (key=null) is not specified by the producer, messages are distributed evenly across partitions in a topic. This means messages are sent in a round-robin fashion (partition p0 then p1 then p2, etc... then back to p0 and so on...).\n",
    "\n",
    "If a key is sent (key != null), then all messages that share the same key will always be sent and stored in the same Kafka partition. A key can be anything to identify a message - a string, numeric value, binary value, etc.\n",
    "\n",
    "Kafka message keys are commonly used when there is a need for message ordering for all messages sharing the same field. For example, in the scenario of tracking trucks in a fleet, we want data from trucks to be in order at the individual truck level. In that case, we can choose the key to be truck_id. In the example shown below, the data from the truck with id truck_id_123 will always go to partition p0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47c1e0f5-0f95-4a25-9387-ef884ed474eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The Kafka Ecosystem**\n",
    "A number of additional tools and libraries have been developed for Kafka over the years to expand its functionality.\n",
    "\n",
    "- _**Kafka Streams**_?\n",
    "Once we have produced data from external systems into Kafka, we may want to process them using stream processing applications. Stream processing applications make use of streaming data stores like Apache Kafka to provide real-time analytics.\n",
    "In that case, we can leverage the **Kafka Streams library**, which is a stream processing framework that is released alongside Apache Kafka. Alternatives you may have heard of for Kafka Streams are **Apache Spark, or Apache Flink.**\n",
    "\n",
    "- _**Kafka Connect**_?\n",
    "Kafka Connect is a tool that allows us to integrate popular systems(sources ad sinks) with Kafka. It allows us to re-use existing components to source data into Kafka and sink data out from Kafka into other data store\n",
    "\n",
    "\n",
    "- **Schema Registry**?\n",
    "The Schema Registry helps register data schemas in Apache Kafka and ensure that producers and consumers will be compatible with each other while evolving. It supports the Apache Avro, Protobuf and JSON-schema data formats.\n",
    "Without a schema registry, producers and consumers are at the risk of breaking when the data schema changes.\n",
    "\n",
    "- _**ksqlDB**_\n",
    "Stream processing database that provides a SQL-like interface to transform Kafka topics and perform common database-like operations such as joins, aggregates, filtering, and other forms of data manipulation on streaming data. Behind the scenes, the ksqlDB webserver translates the SQL commands into a series of Kafka Streams applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33c06d7e-e0e5-4cde-84a8-a08d98e1a9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Apache Kafka components such as:\n",
    "\n",
    "- Kafka Topics\n",
    "- Kafka Producer\n",
    "- Kafka Consumers\n",
    "- Kafka Consumer Groups and Consumer Offsets\n",
    "- Kafka Brokers\n",
    "- Kafka Topic Replication\n",
    "- Zookeeper\n",
    "- KRaft Mode"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Kafka 2.0",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}